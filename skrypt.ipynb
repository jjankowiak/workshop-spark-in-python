{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warsztaty Apache Spark - 19.04.2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vagrant'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# to jest ścieżka w VM\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.bash_logout',\n",
       " 'spark_notebook.py',\n",
       " 'spark_tutorial.ipynb',\n",
       " '.cache',\n",
       " 'LingaroSparkSQL.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " '.bashrc',\n",
       " 'spark_mooc_version',\n",
       " '.config',\n",
       " 'data',\n",
       " '.ipython',\n",
       " '.ssh',\n",
       " '.profile']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista plików i folderów \n",
    "os.listdir('.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rozpoczęcie pracy**\n",
    "Na tej wirtualnej maszynie jest już wszystko skonfigurowane i `SparkContext` jest już zaimportowany i utworzony jako `sc`. Jednak gdyby się okazało, że sami musimy skonfigurować Sparka (gdy nie działamy lokalnie, tylko chcemy się połączyć z klastrem) to robimy to następująco:\n",
    "\n",
    "Importujemy wymagane pakiety (klasy)\n",
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "```\n",
    "Określamy konfiguracje Sparka, które zawierają informacje o aplikacji. `appName` to nazwa aplikacji, która będzie widoczna w UI, `master` to url do klastra. \n",
    "\n",
    "```python\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "U nas wszystko jest już skonfigurowane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test**\n",
    "`SparkContext` to klasa, w której utworzony obiekt mówi Sparkowi jak dostać się do klastra. My mamy już obiekt tej klasy automatycznie utworzony pod nazwą `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzamy typ (klasę) obiektu sc\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'clearFiles',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'environment',\n",
       " 'getLocalProperty',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# atrybuty obiektu sc\n",
    "dir(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas interesować będą tylko dwie metody - `parallelize` oraz `textFile`. Służą one do tworzenia RDD odpowiednio poprzez rozproszenie istniejącego już pliku lub wczytania pliku z zewnętrznych zasobów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tworzenie prostego RDD**\n",
    "Na początek utworzymy RDD wykorzystując istniejący już obiekt (listę pythonową). Do tego służy nam metoda `parallelize`. Możemy przeczytać o niej w pomocy.\n",
    "Zbiór jest kopiowany i przetwarzany na postać RDD. W ten sposób możemy na nim wykonywać operacje równolegle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(self, c, numSlices=None) method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using xrange\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algument `c` to liczba partycji, na jakie ma zostać podzielony zbiór. Jeżeli go nie podamy Spark zrobi to automatycznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberList = [12, 45, 9, 8, 66, 34, 89, 56, 2,  8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberRDD = sc.parallelize(numberList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:392\n"
     ]
    }
   ],
   "source": [
    "# nie widać co tam jest...\n",
    "print numberRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzamy typ\n",
    "type(numberRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDF',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista atrybutów dla RDD - dużo akcji i transformacji\n",
    "dir(numberRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Akcje i transformacje**\n",
    "Akcje to operacje, które zwracają nam wynik, tzn. zwraca obiekt innego typu niż RDD. Transformacje jak sama nazwa wskazuje są operacjami, które tworzą nowe RDD przekształcając już istniejące. Transformacje nie są wykonywane dopóki nie muszą, to znaczy gdy nie następuje po nich akcja.\n",
    "\n",
    "Żeby zobaczyć wynik transformacji musimy wykonać akcję. Może się to wydawać nieintuicyjne, ale to ma sens. Na przykład wczytujemy plik, przefiltrujemy linijmy wg danego kryterium i chcemy zwrócić pierwszą linijkę wyniku. Spark nie wczyta więc nawet całego pliku, nie będzie filtrował wszystkich linijek, zamiast tego będzie wczytywał po linijce i filtrował dopóki nie znajdzie pierwszej linii, która odpowiada warunkowi i ją zwróci po czym skończy obliczenia.\n",
    "\n",
    "## Akcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()\n",
    "Zwraca *wszystkie* elementy zbioru jako tablicę. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 9, 8, 66, 34, 89, 56, 2, 8]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista pythonowa!\n",
    "type(numberRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czyli wykonujemy sobie na RDD różne operacje, szybko i równolegle. Gdy chcemy spowrotem mieć zbiór jako \"zwyczajny\" obiekt żeby do zapisać na dysk itd. to wtedy robimy akcję `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(num), first()\n",
    "Analogicznie jak `collect` z tym że teraz nie zwracamy wszystkiego ale odpwiednio kilka pierwszych wierszy lub pojedynczy pierwszy lub ostatni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 9]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 9, 8, 66, 34, 89, 56, 2, 8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jeżeli podamy za dużą liczbę to zwróci wszystkie\n",
    "numberRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample(withReplacement, num, [seed]), takeOrdered(n, [ordering)\n",
    "`takeSample()` zwraca losową próbę z kolekcji. Pierwszy argument to indykator czy losować ze zwracaniem czy bez, drugi to wielkośc próby, trzeci (nie obligatoryjny) to ziarno.\n",
    "\n",
    "`takeOrdered()` wzraca $n$ (gdzie $n$ to argument) pierwszych elementów uporządkowanych wg naturalnego porządku. Jako drugi argument możemy podać funkcję porządkową (jeżeli chcemy mieć inny porządek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 34, 56]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 8, 9]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zwykłe sortowanie\n",
    "numberRDD.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89, 66, 56, 45, 34]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# porządek malejący - trochę nieintuicyjnie\n",
    "numberRDD.takeOrdered(5, lambda x: -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()\n",
    "Zwraca ilość elementów, które są w kolekcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce(func)\n",
    "Agreguje elementy kolekcji względem funkcji danej jako argument (funkcja przyjmuje dwa argumenty a zwraca jedną wartość). Mamy raczej bardzo ograniczoną liczbę funkcji - muszą to być funkcje agregujące."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suma elementów\n",
    "numberRDD.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy poprawność operując na pythonowej liście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(numberList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### map(func)\n",
    "Zwraca nowe RDD utworzone poprzez wykonanie na każdym elemencie kolekcji funkcji danej w argumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_two(x):\n",
    "    return x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numberRDD_addtwo = numberRDD.map(add_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 47, 11, 10, 68, 36, 91, 58, 4, 10]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD_addtwo.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy też zastosować funkcje lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wskazówka** - akcje i transformacje możemy wykonywać łańcuchowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 90, 18, 16, 132, 68, 178, 112, 4, 16]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.map(lambda x: x * 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap(func)\n",
    "Działa podobnie jak `map()`, ale funkcja dane w argumencie może zwracać np. listę zamiast pojedynczej wartości. Wtedy wyniki są \"spłaszczane\", tzn. scalane w jedną liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_two_three(x):\n",
    "    return [x, x * 2, x * 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 24, 36, 45, 90, 135]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.flatMap(one_two_three).take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(otherDataset), intersection(otherDataset)\n",
    "Funkcje zwracają odpowiednio sumę i przekrój RDD i innego RDD podanego jako argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 9, 8, 66, 34, 89, 56, 2, 8, 'ala', 'ma', 'kota']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.union(sc.parallelize([\"ala\", \"ma\", \"kota\"])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 12]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.intersection(sc.parallelize([12,8,33])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(func)\n",
    "Zwraca przefiltowany zbiór danych, tzn. tylko elementy, dla których wykonana na nich funkcja w argumencie zwraca wartość `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 9, 89]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tylko nieparzyste liczby\n",
    "numberRDD.filter(lambda x: x%2 != 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** Zwróć tylko liczby dwucyfrowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 66, 34, 89, 56]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.filter(lambda x: len(str(x)) == 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()\n",
    "Zwraca RDD bez duplikatów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 45, 9, 8, 66, 34, 89, 56, 2, 8]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 powtarza się dwa razy\n",
    "numberRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 8, 9, 2, 12, 34, 45, 56, 89]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Caching (persisting)**\n",
    "Wcześniej wspomnieliśmy, że RDD są leniwie transformowane, tzn. transformacja nie wykona się dopóki nie wykonamy po niej akcji. Może sę jednak zdarzyć, że RDD utworzone po transformacji chcemy użyć kilkukrotnie (raz od razu wykonać np. akcję `collect()` a za drugim dokonać po tej transformacji jeszcze kilka i dopiero potem wykonać akcję).\n",
    "\n",
    "Spark zrobi więc tak: za każdym razem gdy wywołamy akcję będzie wykonywał obliczenia związane z daną transformacją. To może być szczególnie niewydajne przy iteratywnych algorytmach. Co możemy zrobić? Żeby uniknąć wielokrotnych obliczeń tego samego, wykonujemy operaję `cache()` lub `persist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zostanie wykonana tylko raz i gdy trzeba będzie jej użyć ponownie będzie już \"policzona\"\n",
    "numberRDD_distinct = numberRDD.distinct().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pary klucz-wartość**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacje powyżej są zupełnie podstawowe. Teraz zajmiemy się RDD, które zawierają pary (klucz, wartość), na których możemy wykonywać szereg dodatkowych transformacji (sortowanie i grupowanie po kluczu lub wartość itd.)\n",
    "\n",
    "Popularna jest np. ekstrakcja z RDD części określającej klucz, np. customerID, czas zdarzenia...) i użycie jej jako klucz. W ten sposób możemy wykonywać operacje np. na zgrupowanych wg klucza danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordList = [\"ala\", \"ma\", \"kota\", \"kot\", \"ma\", \"ale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordRDD = sc.parallelize(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzymy sobie pair RDD, tzn. RDD, w których każdy element jest kropką postaci $(klucz, wartość)$. W poniższym przykładzie utworzymy RDD, które dla każdego słowa zwróci krotkę $(słowo, 1)$.\n",
    "\n",
    "\n",
    "** Zadanie ** Utwórz wordPairsRDD, które jest przekształceniem wordRDD, w taki sposób, ze każdemu słowu przypisuje krotkę $(słowo, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordPairsRDD = wordRDD.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', 1), ('ma', 1), ('kota', 1), ('kot', 1), ('ma', 1), ('ale', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordPairsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy policzyć ile razy dane słowo pojawiło się w RDD. Mamy kilka sposobów żeby to zrobić."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()\n",
    "Dla pary $(K, V)$ zwraca $(K, iterable<V>)$, gdzie $iterable<V>$ to obiekt, po którym można iterować utworzony z wartości dla danego klucza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordGroupedRDD = wordPairsRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', <pyspark.resultiterable.ResultIterable at 0xb0f41aec>),\n",
       " ('ale', <pyspark.resultiterable.ResultIterable at 0xb0f417ec>),\n",
       " ('ma', <pyspark.resultiterable.ResultIterable at 0xb0f41f2c>),\n",
       " ('kot', <pyspark.resultiterable.ResultIterable at 0xb0f418cc>),\n",
       " ('kota', <pyspark.resultiterable.ResultIterable at 0xb0f417cc>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordGroupedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej mamy na razie tylko definicję, że to jest obiekt po którym można iterować. Jak to wyświetlić?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ala [1]\n",
      "ale [1]\n",
      "ma [1, 1]\n",
      "kot [1]\n",
      "kota [1]\n"
     ]
    }
   ],
   "source": [
    "for key, value in wordGroupedRDD.collect():\n",
    "    print key, list(value) # z obiektu iterującego roimy listę"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Korzystając z `wordGroupedRDD` utworzyć RDD w postaci kolekcji krotek (słowo, suma wystąpień)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCountRDD = wordGroupedRDD.map(lambda (k, v): (k, sum(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', 1), ('ale', 1), ('ma', 2), ('kot', 1), ('kota', 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey(func)\n",
    "Dla pary $(K, V)$ zwraca zbiór par $(K, V')$, gdzie $V'$ to zbiór powstały poprzez transformacje $V$ funkcją agregacji daną w argumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', 1), ('ale', 1), ('ma', 2), ('kot', 1), ('kota', 1)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordPairsRDD.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey([ascending])\n",
    "Zwraca RDD, które jest posortowane wg klucza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', 1), ('ale', 1), ('kot', 1), ('kota', 1), ('ma', 2)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountRDD.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ma', 2), ('kota', 1), ('kot', 1), ('ale', 1), ('ala', 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w porządku malejącym\n",
    "wordCountRDD.sortByKey(ascending = False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Policz liczbę unikalnych słów w wordRDD. Można korzystać z innych już utworzonych RDD i wcześniej poznanych transformacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# najlepiej skorzystać z wordCountRDD, bo mamy zliczoną ilośc wystąpień danego słowa i zastosować filter\n",
    "wordCountRDD.filter(lambda (x, y): y == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza pliku tekstowego\n",
    "Za pomoca poznanych funkcji będziemy analizować Sonety Szekspira znajdującego się w pliku `shakespeare.txt`. Najpierw musimy utworzyć z pliku tekstowego RDD. Do tego służy nam  metoda SparkCntext `textFile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cs100/lab1/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fileDir = os.path.join('data', 'cs100', 'lab1', 'shakespeare.txt')\n",
    "print fileDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakeRDD = sc.textFile(fileDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(shakeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'',\n",
       " u'THE SONNETS',\n",
       " u'',\n",
       " u'by William Shakespeare',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'                     1',\n",
       " u'  From fairest creatures we desire increase,',\n",
       " u\"  That thereby beauty's rose might never die,\",\n",
       " u'  But as the riper should by time decease,',\n",
       " u'  His tender heir might bear his memory:',\n",
       " u'  But thou contracted to thine own bright eyes,',\n",
       " u\"  Feed'st thy light's flame with self-substantial fuel,\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzamy w jaki sposób RDD zostało utworzone\n",
    "shakeRDD.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Policz ile linii ma utwór."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122395"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Policz ile niepustych linii ma utwór i nadpisz tak uwtorzone RDD do `shakeRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112902"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeRDD.filter(lambda x: x != '').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakeRDD = shakeRDD.filter(lambda x: x != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'THE SONNETS',\n",
       " u'by William Shakespeare',\n",
       " u'                     1',\n",
       " u'  From fairest creatures we desire increase,',\n",
       " u\"  That thereby beauty's rose might never die,\",\n",
       " u'  But as the riper should by time decease,',\n",
       " u'  His tender heir might bear his memory:',\n",
       " u'  But thou contracted to thine own bright eyes,',\n",
       " u\"  Feed'st thy light's flame with self-substantial fuel,\",\n",
       " u'  Making a famine where abundance lies,',\n",
       " u'  Thy self thy foe, to thy sweet self too cruel:',\n",
       " u\"  Thou that art now the world's fresh ornament,\",\n",
       " u'  And only herald to the gaudy spring,',\n",
       " u'  Within thine own bud buriest thy content,']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzamy\n",
    "shakeRDD.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Usuń znaki interpunkcyjne, sprowadź tekst do małych liter i podziel zbiór na słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print string.punctuation # wszystkie znaki interpunkcyjnev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakeWordsRDD = shakeRDD.map(remove_punct)\\\n",
    "                        .map(lambda x: x.lower())\\\n",
    "                        .map(lambda x : x.split())\\\n",
    "                        .flatMap(lambda x: x)# lista list ze slowami -> jedna duża lista ze słowami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'the',\n",
       " u'sonnets',\n",
       " u'by',\n",
       " u'william',\n",
       " u'shakespeare',\n",
       " u'1',\n",
       " u'from',\n",
       " u'fairest',\n",
       " u'creatures',\n",
       " u'we',\n",
       " u'desire',\n",
       " u'increase',\n",
       " u'that',\n",
       " u'thereby',\n",
       " u'beautys',\n",
       " u'rose',\n",
       " u'might',\n",
       " u'never',\n",
       " u'die']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeWordsRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Stwórz RDD, które będzie zawierało krotki $(słowo, liczba\\_wystąpień)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakePairsRDD = shakeWordsRDD.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Uporządkuj słowa wg ilości wystąpień, wyświetl pierwsze 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 27361),\n",
       " (u'and', 26028),\n",
       " (u'i', 20681),\n",
       " (u'to', 19150),\n",
       " (u'of', 17463),\n",
       " (u'a', 14593),\n",
       " (u'you', 13615),\n",
       " (u'my', 12481),\n",
       " (u'in', 10956),\n",
       " (u'that', 10890),\n",
       " (u'is', 9134),\n",
       " (u'not', 8497),\n",
       " (u'with', 7771),\n",
       " (u'me', 7769),\n",
       " (u'it', 7678),\n",
       " (u'for', 7558),\n",
       " (u'be', 6857),\n",
       " (u'his', 6857),\n",
       " (u'your', 6655),\n",
       " (u'this', 6602)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakePairsRDD.takeOrdered(20, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drugi sposób, jednak brzydki, bo co gdy nowe klucze (czyli liczba wystąpień) nie jest unikalna?\n",
    "# musimy tymczasowo zamienić miejscami klucze i wartości \n",
    "shakeSortedRDD = shakePairsRDD.map(lambda (x, y): (y, x)).sortByKey(ascending = False).map(lambda (x, y): (y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 27361),\n",
       " (u'and', 26028),\n",
       " (u'i', 20681),\n",
       " (u'to', 19150),\n",
       " (u'of', 17463),\n",
       " (u'a', 14593),\n",
       " (u'you', 13615),\n",
       " (u'my', 12481),\n",
       " (u'in', 10956),\n",
       " (u'that', 10890),\n",
       " (u'is', 9134),\n",
       " (u'not', 8497),\n",
       " (u'with', 7771),\n",
       " (u'me', 7769),\n",
       " (u'it', 7678),\n",
       " (u'for', 7558),\n",
       " (u'be', 6857),\n",
       " (u'his', 6857),\n",
       " (u'your', 6655),\n",
       " (u'this', 6602)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeSortedRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Do krotki dołącz trzecią współrzędną, ktora jest równa `True` jeżeli słowo jest stopwordem i `False` w przeciwnym przypadku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cs100/lab3/stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "# wczytajmy najpierw plik zawierający stopwordsy\n",
    "import os\n",
    "fileDir = os.path.join('data', 'cs100', 'lab3', 'stopwords.txt')\n",
    "print fileDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = sc.textFile(fileDir).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakeStopRDD = shakeSortedRDD.map(lambda (x, y): (x, y, True) if x in stopwords else (x, y, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 27361, True),\n",
       " (u'and', 26028, True),\n",
       " (u'i', 20681, True),\n",
       " (u'to', 19150, True),\n",
       " (u'of', 17463, True),\n",
       " (u'a', 14593, True),\n",
       " (u'you', 13615, True),\n",
       " (u'my', 12481, True),\n",
       " (u'in', 10956, True),\n",
       " (u'that', 10890, True),\n",
       " (u'is', 9134, True),\n",
       " (u'not', 8497, True),\n",
       " (u'with', 7771, True),\n",
       " (u'me', 7769, True),\n",
       " (u'it', 7678, True)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeStopRDD.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bez funkcji lambda\n",
    "def third_coord(x):\n",
    "    if x[0] in stopwords:\n",
    "        return (x[0], x[1], True)\n",
    "    else:\n",
    "        return (x[0], x[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 27361, True),\n",
       " (u'and', 26028, True),\n",
       " (u'i', 20681, True),\n",
       " (u'to', 19150, True),\n",
       " (u'of', 17463, True),\n",
       " (u'a', 14593, True),\n",
       " (u'you', 13615, True),\n",
       " (u'my', 12481, True),\n",
       " (u'in', 10956, True),\n",
       " (u'that', 10890, True),\n",
       " (u'is', 9134, True),\n",
       " (u'not', 8497, True),\n",
       " (u'with', 7771, True),\n",
       " (u'me', 7769, True),\n",
       " (u'it', 7678, True)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeSortedRDD.map(third_coord).take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Policz ile procent wszystkich słów to stopwordsy:\n",
    "- procent uniklanych stopwordsów wśród uniklanych słów\n",
    "- procent jako suma wszystkich stopwordsow/suma wszyskich slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeStopRDD.filter(lambda (x, y, z): z == True).count()/shakeStopRDD.count() # python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004512026148435002"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(shakeStopRDD.filter(lambda (x, y, z): z == True).count())/shakeStopRDD.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drugie\n",
    "ileStopwords = shakeStopRDD.filter(lambda (x, y, z): z == True).map(lambda (x, y, z): y).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ileAll = shakeStopRDD.map(lambda (x, y, z): y).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46270198279493907"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(ileStopwords)/ileAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Usuń stopwordsy i wyświetl jeszcze raz 20 najpopularniejszych i najrzadszych słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakeWithoutStopRDD = shakeStopRDD.filter(lambda (x, y, z): z == False).map(lambda (x, y, z): (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'thou', 5485),\n",
       " (u'thy', 4032),\n",
       " (u'shall', 3591),\n",
       " (u'thee', 3178),\n",
       " (u'lord', 3059),\n",
       " (u'king', 2861),\n",
       " (u'good', 2812),\n",
       " (u'sir', 2754),\n",
       " (u'o', 2607),\n",
       " (u'come', 2507),\n",
       " (u'well', 2462),\n",
       " (u'would', 2293),\n",
       " (u'let', 2099),\n",
       " (u'enter', 2098),\n",
       " (u'love', 2053),\n",
       " (u'ill', 1972),\n",
       " (u'hath', 1941),\n",
       " (u'man', 1835),\n",
       " (u'one', 1779),\n",
       " (u'go', 1733)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeWithoutStopRDD.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'redeemst', 1),\n",
       " (u'chameleons', 1),\n",
       " (u'offendeth', 1),\n",
       " (u'beadsmen', 1),\n",
       " (u'opener', 1),\n",
       " (u'swoopstake', 1),\n",
       " (u'slothful', 1),\n",
       " (u'appropriation', 1),\n",
       " (u'selfreproving', 1),\n",
       " (u'sooty', 1),\n",
       " (u'roundwombd', 1),\n",
       " (u'maythat', 1),\n",
       " (u'paphos', 1),\n",
       " (u'razeth', 1),\n",
       " (u'committst', 1),\n",
       " (u'sunbeams', 1),\n",
       " (u'china', 1),\n",
       " (u'climbed', 1),\n",
       " (u'stringless', 1),\n",
       " (u'ycleped', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeWithoutStopRDD.takeOrdered(20, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Spark SQL **\n",
    "Spark SQL zapewnia dwie podstawowe funkcjonalności:\n",
    "1. Wprowadza pojęcie `DataFrame`, który to obiekt ułatwia pracowanie z ustrukturyzowanymi zbiorami. `DataFrame` możemy utożsamiać z tabelą w bazie danych.\n",
    "2. Ma możliwosć czytania danych z różnych źródeł i formatów (JSON, Hive Tables, przekształcenie obiektu pythonowego)\n",
    "\n",
    "Żeby rozpocząć pracę z SQL w Sparku trzeba utworzyć `SQLContext`. `SQLContext` opakowuje `SparkContext`, dzięki temu mamy dodatkowe funkcje, które pozwalają nam pracować z ustrukturyzowanymi danymi. Możemy utworzyć też `HQLContext`, którego funkcjonalność jest rozszerzona (możemy korzystać z HiveSQ i czytać tabelki Hive).\n",
    "\n",
    "Za pomocą `SQLContext` możemy utworzyć DataFrame, czyli obiekt na którym dalej będzie wykonywać operacje SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie DataFrame\n",
    "`DataFrame` zawiera obiekty RDD typu `Row`, każdy z nich reprezentuje rekord. `DataFrames` przechowują dane efektywnie niż \"zwykłe RDD\", zatem jeżeli znamy schemat danych to warto ich używać. Ponadto, ramki zapewniają nowe operacje niedostępne pod zwykłym RDD, takie jak np. zapytania SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### moviesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "moviesPath = os.path.join(\"data\", \"cs100\", \"lab4\", \"small\", \"movies.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"1::Toy Story (1995)::Animation|Children's|Comedy\",\n",
       " u\"2::Jumanji (1995)::Adventure|Children's|Fantasy\",\n",
       " u'3::Grumpier Old Men (1995)::Comedy|Romance',\n",
       " u'4::Waiting to Exhale (1995)::Comedy|Drama',\n",
       " u'5::Father of the Bride Part II (1995)::Comedy']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# najpierw tworzymy sobie zwyczajnie RDD\n",
    "moviesRDD = sc.textFile(moviesPath)\n",
    "# każdy wiersz to string, komórki oddzielone ciągiem znaków \"::\"\n",
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# podzielmy najpierw każdy wiersz na 3 części względem \"::\"\n",
    "moviesRDD = moviesRDD.map(lambda x: x.split(\"::\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1', u'Toy Story (1995)', u\"Animation|Children's|Comedy\"],\n",
       " [u'2', u'Jumanji (1995)', u\"Adventure|Children's|Fantasy\"],\n",
       " [u'3', u'Grumpier Old Men (1995)', u'Comedy|Romance'],\n",
       " [u'4', u'Waiting to Exhale (1995)', u'Comedy|Drama'],\n",
       " [u'5', u'Father of the Bride Part II (1995)', u'Comedy']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importujemy Row i przekształcamy każdy wiersz na obiket typu Row - przy okazji zmieniamy typ danych na odpowiedni\n",
    "from pyspark.sql import Row\n",
    "moviesRows = moviesRDD.map(lambda x: Row(movieid = int(x[0]), title = x[1], genre = x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(genre=u\"Animation|Children's|Comedy\", movieid=1, title=u'Toy Story (1995)'),\n",
       " Row(genre=u\"Adventure|Children's|Fantasy\", movieid=2, title=u'Jumanji (1995)'),\n",
       " Row(genre=u'Comedy|Romance', movieid=3, title=u'Grumpier Old Men (1995)'),\n",
       " Row(genre=u'Comedy|Drama', movieid=4, title=u'Waiting to Exhale (1995)'),\n",
       " Row(genre=u'Comedy', movieid=5, title=u'Father of the Bride Part II (1995)')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to jest wciąż RDD\n",
    "moviesRows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wreszcie tworzymy DataFrame\n",
    "moviesDF = sqlContext.createDataFrame(moviesRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre                movieid title               \n",
      "Animation|Childre... 1       Toy Story (1995)    \n",
      "Adventure|Childre... 2       Jumanji (1995)      \n",
      "Comedy|Romance       3       Grumpier Old Men ...\n",
      "Comedy|Drama         4       Waiting to Exhale...\n",
      "Comedy               5       Father of the Bri...\n"
     ]
    }
   ],
   "source": [
    "moviesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(moviesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ratingsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** Stwórz DataFrame z pliku ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsPath = os.path.join(\"data\", \"cs100\", \"lab4\", \"small\", \"ratings.dat.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plik ratings.dat jest spakowany, więc musimy go wczytać następująco\n",
    "import gzip\n",
    "gzf = gzip.GzipFile(ratingsPath);\n",
    "ratingsList = gzf.readlines();\n",
    "gzf.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760\\n', '1::661::3::978302109\\n', '1::914::3::978301968\\n']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podpowiedz - nazwy kolumn to userid, movieid, rating, timestamp\n",
    "ratingsList[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratingsList jest listą (istniejący już obiekt więc korzystamy z parallelize\n",
    "ratingsRDD = sc.parallelize(ratingsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760\\n',\n",
       " '1::661::3::978302109\\n',\n",
       " '1::914::3::978301968\\n',\n",
       " '1::3408::4::978300275\\n',\n",
       " '1::2355::5::978824291\\n']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsRDD = ratingsRDD.map(lambda x: x.split(\"::\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '1193', '5', '978300760\\n'],\n",
       " ['1', '661', '3', '978302109\\n'],\n",
       " ['1', '914', '3', '978301968\\n'],\n",
       " ['1', '3408', '4', '978300275\\n'],\n",
       " ['1', '2355', '5', '978824291\\n']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# odrzucamy ostatnią kolumnę, gdyż nie jest nam potrzebna\n",
    "ratingsRows = ratingsRDD.map(lambda x: Row(userid = int(x[0]), movieid = int(x[1]), rating = float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieid=1193, rating=5.0, userid=1),\n",
       " Row(movieid=661, rating=3.0, userid=1),\n",
       " Row(movieid=914, rating=3.0, userid=1),\n",
       " Row(movieid=3408, rating=4.0, userid=1),\n",
       " Row(movieid=2355, rating=5.0, userid=1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsDF = sqlContext.createDataFrame(ratingsRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid rating userid\n",
      "1193    5.0    1     \n",
      "661     3.0    1     \n",
      "914     3.0    1     \n",
      "3408    4.0    1     \n",
      "2355    5.0    1     \n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapytania SQL\n",
    "Operacje na DataFrame możemy wykonywać na dwa sposoby (analogicznie jak uczyliśmy się na zajęciach z Pythona), tzn. korzystając z zapytania SQL podanego jako string lub z metod, które mają ramki danych.\n",
    "\n",
    "Żeby korzystać z języka SQL musimy \"zarejestrować\" tabelkę. Natomiat jeżeli chcemy korzystać z metod Data Frame nie musimy tego robić."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moviesDF.registerTempTable(\"movies\")\n",
    "ratingsDF.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0   \n",
      "487650\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sqlContext.sql('SELECT COUNT(*) FROM ratings').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0 \n",
      "3883\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT COUNT(*) FROM movies').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid\n",
      "1      \n",
      "2      \n",
      "3      \n",
      "4      \n",
      "5      \n"
     ]
    }
   ],
   "source": [
    "moviesDF.select(\"movieid\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid title               \n",
      "1       Toy Story (1995)    \n",
      "2       Jumanji (1995)      \n",
      "3       Grumpier Old Men ...\n",
      "4       Waiting to Exhale...\n",
      "5       Father of the Bri...\n"
     ]
    }
   ],
   "source": [
    "moviesDF.select(\"movieid\", \"title\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre                movieid title               \n",
      "Animation|Childre... 1       Toy Story (1995)    \n",
      "Adventure|Childre... 2       Jumanji (1995)      \n",
      "Comedy|Romance       3       Grumpier Old Men ...\n",
      "Comedy|Drama         4       Waiting to Exhale...\n",
      "Comedy               5       Father of the Bri...\n"
     ]
    }
   ],
   "source": [
    "moviesDF.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(movieid + 1) title               \n",
      "2             Toy Story (1995)    \n",
      "3             Jumanji (1995)      \n",
      "4             Grumpier Old Men ...\n",
      "5             Waiting to Exhale...\n",
      "6             Father of the Bri...\n"
     ]
    }
   ],
   "source": [
    "# jeżeli chcemy wybrać np. movieid ale jakos go zmodyfikować, wtedy piszemy następująco\n",
    "moviesDF.select(moviesDF[\"movieid\"] + 1, \"title\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid rating userid\n",
      "1193    5.0    1     \n",
      "2355    5.0    1     \n",
      "1287    5.0    1     \n",
      "2804    5.0    1     \n",
      "595     5.0    1     \n"
     ]
    }
   ],
   "source": [
    "ratingsDF.filter(ratingsDF[\"rating\"] >= 4.5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid\n",
      "1193   \n",
      "2355   \n",
      "1287   \n",
      "2804   \n",
      "595    \n"
     ]
    }
   ],
   "source": [
    "# oczywiście operacje możemy wkonywać łańcuchowo\n",
    "ratingsDF.filter(ratingsDF[\"rating\"] >= 4.5).select(\"movieid\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy()\n",
    "Po tym musi nastąpić agregacja np. `min()`, `max()`, `mean()` czy `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating count \n",
      "1.0    27472 \n",
      "3.0    127216\n",
      "5.0    108545\n",
      "4.0    170579\n",
      "2.0    53838 \n"
     ]
    }
   ],
   "source": [
    "# po groupBy musimy użyć jakiejś metody agregującej\n",
    "ratingsDF.groupBy(\"rating\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre               \n",
      "Action|Adventure|...\n",
      "Action|Adventure|...\n",
      "Action|Drama|Thri...\n",
      "Animation|Childre...\n",
      "Action|Adventure|...\n",
      "Adventure|Animati...\n",
      "Action|Drama        \n",
      "Comedy|Horror|Mus...\n",
      "Comedy|Documentary  \n",
      "Action|Adventure|...\n",
      "Comedy|Drama        \n",
      "Children's|Comedy...\n",
      "Children's|Fantas...\n",
      "Comedy|Crime|Fantasy\n",
      "Crime|Film-Noir     \n",
      "Action|Comedy|Drama \n",
      "Adventure|Comedy    \n",
      "Adventure|Fantasy...\n",
      "Fantasy             \n",
      "Adventure|Comedy|...\n"
     ]
    }
   ],
   "source": [
    "moviesDF.select(\"genre\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre                movieid title               \n",
      "Children's|Comedy    2031    $1,000,000 Duck (...\n",
      "Drama                3112    'Night Mother (1986)\n",
      "Drama|Romance        779     'Til There Was Yo...\n",
      "Comedy               2072    'burbs, The (1989)  \n",
      "Drama|Thriller       3420    ...And Justice fo...\n",
      "Romance              889     1-900 (1994)        \n",
      "Comedy|Romance       2572    10 Things I Hate ...\n",
      "Animation|Children's 2085    101 Dalmatians (1...\n",
      "Children's|Comedy    1367    101 Dalmatians (1...\n",
      "Drama                1203    12 Angry Men (1957) \n"
     ]
    }
   ],
   "source": [
    "moviesDF.orderBy(\"title\").limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()\n",
    "Zwraca RDD poprzez zastosowanie do każdego wiersza funkcji dane w argumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u'Animation'),\n",
       " (2, u'Adventure'),\n",
       " (3, u'Comedy'),\n",
       " (4, u'Comedy'),\n",
       " (5, u'Comedy')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tylko pierwszy gatunek filmu\n",
    "# to jest znów zwykłe RDD, więc musielibyśmy utworzyć z tego nową ramkę danych\n",
    "moviesDF.map(lambda x: (x.movieid, x.genre.split(\"|\")[0])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** Wykonaj te same operacje co w SQL za pomocą metod dla `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0\n",
      "843\n"
     ]
    }
   ],
   "source": [
    "# liczba filmów oznaczone jako dramat (tylko jeden gatunek)\n",
    "sqlContext.sql(\"select COUNT(*) from movies where genre=='Drama'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843L"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesDF.filter(\"genre = 'Drama'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843L"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesDF.filter(moviesDF.genre == 'Drama').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0  \n",
      "1603\n"
     ]
    }
   ],
   "source": [
    "# policz ile jest filmów, które są oznaczone jako dramat (moze być jeszcze jakiś inny gatunek)\n",
    "sqlContext.sql(\"select COUNT(*) from movies where genre like '%Drama%' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603L"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesDF.filter(\"genre like '%Drama%'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** Korzystając z tabeli ratingsDF pogrupuj filmy wg moviesID i policz srednia ocen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultSQL = sqlContext.sql(\"SELECT movieID, avg(rating) AS avg_rating FROM ratings GROUP BY movieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieID avg_rating        \n",
      "3031    2.0869565217391304\n",
      "831     3.7               \n",
      "631     2.0               \n",
      "1031    3.4615384615384617\n",
      "2631    1.0               \n"
     ]
    }
   ],
   "source": [
    "resultSQL.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# za pomocą metod DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_rating = ratingsDF.groupBy(\"movieid\").mean(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid AVG(rating)       \n",
      "3031    2.0869565217391304\n",
      "831     3.7               \n",
      "631     2.0               \n",
      "1031    3.4615384615384617\n",
      "2631    1.0               \n"
     ]
    }
   ],
   "source": [
    "avg_rating.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** Połącz tabelkę ze średnimi ocenami z tytułami filmów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinedDF = avg_rating.join(moviesDF, moviesDF.movieid == ratingsDF.movieid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieid AVG(rating)        genre                movieid title               \n",
      "3031    2.0869565217391304 Comedy               3031    Repossessed (1990)  \n",
      "831     3.7                Drama                831     Stonewall (1995)    \n",
      "631     2.0                Animation|Childre... 631     All Dogs Go to He...\n",
      "1031    3.4615384615384617 Adventure|Childre... 1031    Bedknobs and Broo...\n",
      "2631    1.0                Comedy|Film-Noir|... 2631    Frogs for Snakes ...\n",
      "31      3.2419354838709675 Drama                31      Dangerous Minds (...\n",
      "2031    3.15               Children's|Comedy    2031    $1,000,000 Duck (...\n",
      "3431    2.3333333333333335 Action|Drama         3431    Death Wish II (1982)\n",
      "231     3.1651090342679127 Comedy               231     Dumb & Dumber (1994)\n",
      "2431    3.123222748815166  Comedy|Drama         2431    Patch Adams (1998)  \n"
     ]
    }
   ],
   "source": [
    "joinedDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                AVG(rating)        genre               \n",
      "Repossessed (1990)   2.0869565217391304 Comedy              \n",
      "Stonewall (1995)     3.7                Drama               \n",
      "All Dogs Go to He... 2.0                Animation|Childre...\n",
      "Bedknobs and Broo... 3.4615384615384617 Adventure|Childre...\n",
      "Frogs for Snakes ... 1.0                Comedy|Film-Noir|...\n",
      "Dangerous Minds (... 3.2419354838709675 Drama               \n",
      "$1,000,000 Duck (... 3.15               Children's|Comedy   \n",
      "Death Wish II (1982) 2.3333333333333335 Action|Drama        \n",
      "Dumb & Dumber (1994) 3.1651090342679127 Comedy              \n",
      "Patch Adams (1998)   3.123222748815166  Comedy|Drama        \n"
     ]
    }
   ],
   "source": [
    "joinedDF.select(\"title\", \"AVG(rating)\", \"genre\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
